{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "2020_assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kyfqk1ZwV3we"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "***\n",
        "**Name**: Timothy Mason\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x2HnzA-ZURSu"
      },
      "source": [
        "## Goal\n",
        "The goal of this assignment is to use TensorFlow to build some recurrent neural nets (RNNs) and to understand their limitations through experimentation.\n",
        "\n",
        "## The Task\n",
        "You will implement a recurrent neural net to learn the parity operator. The net will have a single input unit and a single output unit, and a fully-connected layer of H hidden units. The inputs and target outputs are binary. When an input sequence is presented, the output state at the end of the sequence should be a parity bit: output should be 1 if the input has an odd number of '1' values. For example, the sequence 1-0-0-1-0-1 should yield output 1 and the sequence 0-0-0-0-1-1 should yield output 0. Note that a target is given only at the end of each sequence. (Parity is easy to learn if there is a target at each step that indicates parity given the sequence so far.)\n",
        "\n",
        "Parity is a hard problem for neural nets to learn because very similar inputs produce different outputs, and very dissimilar inputs can produce the same output.\n",
        "\n",
        "The aspects of the task we will manipulate are:  H, the number of hidden units, N, the length of the input strings, and the activation function for the hidden units, either tanh or LSTM-style neurons. The output neuron should have a logistic activation function.\n",
        "\n",
        "Tip: get started early.  Depending on your system, these nets can take several minutes to train.  Exploring hyperparameters (such as training rate) will be critical for success.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKEl35tibl8t"
      },
      "source": [
        "### Some Help\n",
        "Below are some helper codes to:\n",
        "- generate input strings and their parity.\n",
        "- provide a callback to trigger early stopping during training\n",
        "- plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-01T07:40:06.367772Z",
          "start_time": "2020-03-01T07:40:06.346997Z"
        },
        "colab_type": "code",
        "id": "Rsf2q8L6UY8n",
        "outputId": "db241781-5906-40d6-c185-8c78de731e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def gen_binary_sequences(num, length):\n",
        "  '''\n",
        "  Generate :num: sequences of length :length:.\n",
        "  '''\n",
        "  return(np.random.randint(0, 2, size=(num, length), dtype=np.int32))\n",
        "\n",
        "def calc_parity(seqs):\n",
        "  '''\n",
        "  Calculate sequence parity (1 if odd number of 1s, 0 if even number of 1s)\n",
        "  '''\n",
        "  return(seqs.sum(axis=-1) % 2)\n",
        "\n",
        "\n",
        "class create_accuracy_callback(tf.keras.callbacks.Callback):\n",
        "    '''\n",
        "    Callback function to stop training at 100% accuracy\n",
        "    '''\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('accuracy') == 1.0 and logs.get('val_accuracy') == 1.0):\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "def plot_accuracies(accuracies,Ns,Hs):\n",
        "  '''\n",
        "  Make a graph of mean % correct (and standard error) on the test set for the different values of H and N.\n",
        "  Input: ndarray of |Hs|x|Ns|x(reps)\n",
        "         actual values of Ns and Hs\n",
        "  '''\n",
        "  lenH,lenN,lenreps = accuracies.shape\n",
        "  assert(lenH == len(Hs))\n",
        "  assert(lenN == len(Ns))\n",
        "  accuracies_mean = accuracies.mean(axis=2)\n",
        "  accuracies_std = accuracies.std(axis=2)\n",
        "  accuracies_stderr = accuracies_std/np.sqrt(lenreps)\n",
        "  # plot\n",
        "  fig = pyplot.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.axhline(0.5,linestyle=\"--\",color=\"gray\") # chance baseline\n",
        "  centers = np.arange(lenN)\n",
        "  for Hindex, H in enumerate(Hs):\n",
        "    ax.bar(centers + 0.8/lenH*(Hindex-(lenH-1)/2),\n",
        "           accuracies_mean[Hindex], 0.8/lenH, yerr=accuracies_stderr[Hindex],\n",
        "           alpha=0.5, label=f\"{H} hidden units\")\n",
        "  ax.set_xlabel(\"sequence length\")\n",
        "  pyplot.xticks(centers,Ns)\n",
        "  ax.set_ylabel(\"accuracy\")\n",
        "  pyplot.legend(loc=\"lower left\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fyhd-HYBdcF0"
      },
      "source": [
        "## Part 1\n",
        "**Part 1.a**<br>\n",
        "Fill in the code to create a net given H and N using the tanh activation function for the hidden units.  Keras has a number of RNN helper functions, although you can also write your own custom layers.\n",
        "\n",
        "Remember that the net should only take one bit of input at a time from the input sequence, and output one logistic value (between 0 and 1) only after the input sequence is complete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-01T07:40:07.952340Z",
          "start_time": "2020-03-01T07:40:07.944403Z"
        },
        "colab_type": "code",
        "id": "AJuSiefzejFZ",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from datetime import datetime\n",
        "\n",
        "# %load_ext tensorboard\n",
        "# # Clear any logs from previous runs\n",
        "# !rm -rf ./logs/ \n",
        "\n",
        "def build_model(N,H):\n",
        "    '''\n",
        "    Builder for an RNN model. Model inputs are binary sequences of length :N:.\n",
        "\n",
        "    At each sequence position, the input and prior state should be fully connected \n",
        "    to :H: hidden units with tanh activation.\n",
        "\n",
        "    The output of the last state of the RNN should be fully connected to a single\n",
        "    unit with logistic activation, to perform the final classification of the \n",
        "    sequence.\n",
        "    '''\n",
        "\n",
        "    # based on example Keras RNN code from https://www.tensorflow.org/guide/keras/rnn\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Embedding(input_dim=N, output_dim=H))\n",
        "    model.add(layers.SimpleRNN(H))   # defaults are good (tanh activation, use a bias vector)\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k0O0WxWqnTD7"
      },
      "source": [
        "**Part 1.b**<br>\n",
        "Then fill in the code to train several such nets.  Each repetition should randomize the initial weights and generate a random training set of 10000 examples of length N as well as a random test set of 10000 examples of length N.  Save 10% of the training as validation, and use at least the provided check_accuracy callback as an early stopping condition.\n",
        "\n",
        "Train nets for H ∈ {5, 25} and for N ∈ {5, 10, 15, 20}.  For each combination of H and N, run 10 replications of your simulation.  You will also need to try to find helpful learning rates; don't be surprised if your training is prone to cycles of stagnation for hundreds of epochs before quickly learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-01T07:40:09.514515Z",
          "start_time": "2020-03-01T07:40:09.508344Z"
        },
        "colab_type": "code",
        "id": "wpOs5URgWsQK",
        "colab": {}
      },
      "source": [
        "BATCH = 1000\n",
        "MAX_EPOCHS = 2000\n",
        "\n",
        "# logdir=\"logs/sequential_fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "def train_model(model, x_train, y_train, x_test, y_test):\n",
        "    '''\n",
        "    Compiles and trains the :model:.\n",
        "    '''\n",
        "    # start by compiling the model; accuracy should be a tracked metric\n",
        "    model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # train model\n",
        "    check_accuracy = create_accuracy_callback()\n",
        "    model.fit( x_train,\n",
        "              y_train, \n",
        "              batch_size=BATCH, \n",
        "              epochs=MAX_EPOCHS, \n",
        "              verbose=0, \n",
        "              validation_data=(x_test, y_test),\n",
        "              use_multiprocessing=True,\n",
        "              callbacks=[check_accuracy] )\n",
        "            #   callbacks=[check_accuracy, tensorboard_callback] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-01T07:40:16.774891Z",
          "start_time": "2020-03-01T07:40:10.678481Z"
        },
        "colab_type": "code",
        "id": "jmLQlzRNfiUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "7422fcac-77fb-42e5-b5c6-89aad26a416b"
      },
      "source": [
        "Hs = [5,25]#  switch to [5,25] when things are working\n",
        "Ns = [5] # switch to [5,10,20,40] when things are working\n",
        "REPS = 10  # switch to 10 when things are working\n",
        "SAMPLES = 10000\n",
        "\n",
        "test_accs = np.zeros([len(Hs),len(Ns),REPS])\n",
        "for Nindex, N in enumerate(Ns):\n",
        "    for rep in range(REPS):\n",
        "        # (data is reusable across changes to the number of hidden units)\n",
        "        # generate a random training set of 10000 examples of length N\n",
        "        x_train = gen_binary_sequences(SAMPLES, N)\n",
        "        y_train = calc_parity(x_train)\n",
        "\n",
        "        # generate a random test set of 10000 examples of length N\n",
        "        x_test = gen_binary_sequences(SAMPLES, N)\n",
        "        y_test = calc_parity(x_test)\n",
        "\n",
        "        for Hindex, H in enumerate(Hs):\n",
        "            ds = datetime.now().strftime(\"%d-%b %T\")\n",
        "            print(f\"{ds}: starting N={N}/{Ns}, H={H}/{Hs}, rep {rep+1}/{REPS}\")\n",
        "            # build model\n",
        "            model = build_model(N,H)\n",
        "            # train model\n",
        "            train_model(model, x_train, y_train, x_test, y_test)\n",
        "            # test model with newly generated test data (extracting test accuracy as test_acc) \n",
        "            x_test = gen_binary_sequences(SAMPLES, N)\n",
        "            y_test = calc_parity(x_test)\n",
        "            check_accuracy = create_accuracy_callback()\n",
        "            test_acc = model.evaluate( x=x_test, \n",
        "                                    y=y_test, \n",
        "                                    batch_size=BATCH, \n",
        "                                    verbose=1, \n",
        "                                    callbacks=[check_accuracy] )[1]\n",
        "                                    # callbacks=[check_accuracy, tensorboard_callback] )[1]\n",
        "\n",
        "            test_accs[Hindex,Nindex,rep] = test_acc"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01-03mm 20:25:24: starting N=5/[5], H=5/[5, 25], rep 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e34ad7a9d536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# test model with newly generated test data (extracting test accuracy as test_acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_binary_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-71fe9caf51d2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     21\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m               callbacks=[check_accuracy] )\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;31m#   callbacks=[check_accuracy, tensorboard_callback] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u4RbGJEbexGV"
      },
      "source": [
        "**Part 1.c**<br>\n",
        "Make the graph of mean % correct (and standard error) on the test set for the different values of H and N."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ew6HWSpkpeW",
        "colab": {}
      },
      "source": [
        "plot_accuracies(test_accs,Ns,Hs)\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ku9WKNAafXnj"
      },
      "source": [
        "## Part 2\n",
        "Repeat the experiments of Part 1, but use LSTM neurons instead of tanh neurons in the recurrent layer.  Comment on your experiences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GU7uynCutpJ7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1NhUtyLry9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_lstm(N,H):\n",
        "    '''\n",
        "    Builder for an LST model. Model inputs are binary sequences of length :N:.\n",
        "\n",
        "    '''\n",
        "\n",
        "    # based on example Keras RNN code from https://www.tensorflow.org/guide/keras/rnn\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Embedding(input_dim=N, output_dim=H))\n",
        "    model.add(layers.LSTM(H))   \n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B47-LQqctBVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_accs_lstm = np.zeros([len(Hs),len(Ns),REPS])\n",
        "for Nindex, N in enumerate(Ns):\n",
        "    for rep in range(REPS):\n",
        "        # (data is reusable across changes to the number of hidden units)\n",
        "        # generate a random training set of 10000 examples of length N\n",
        "        x_train = gen_binary_sequences(SAMPLES, N)\n",
        "        y_train = calc_parity(x_train)\n",
        "\n",
        "        # generate a random test set of 10000 examples of length N\n",
        "        x_test = gen_binary_sequences(SAMPLES, N)\n",
        "        y_test = calc_parity(x_test)\n",
        "\n",
        "        for Hindex, H in enumerate(Hs):\n",
        "            ds = datetime.now().strftime(\"%d-%b %T\")\n",
        "            print(f\"{ds}: starting N={N}/{Ns}, H={H}/{Hs}, rep {rep+1}/{REPS}\")\n",
        "            # build model\n",
        "            model = build_model_lstm(N,H)\n",
        "            # train model\n",
        "            train_model(model, x_train, y_train, x_test, y_test)\n",
        "            # test model with newly generated test data (extracting test accuracy as test_acc) \n",
        "            x_test = gen_binary_sequences(SAMPLES, N)\n",
        "            y_test = calc_parity(x_test)\n",
        "            check_accuracy = create_accuracy_callback()\n",
        "            test_acc = model.evaluate( x=x_test, \n",
        "                                    y=y_test, \n",
        "                                    batch_size=BATCH, \n",
        "                                    verbose=1, \n",
        "                                    callbacks=[check_accuracy] )[1]\n",
        "                                    # callbacks=[check_accuracy, tensorboard_callback] )[1]\n",
        "\n",
        "            test_accs_lstm[Hindex,Nindex,rep] = test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6OreE18tSzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_accuracies(test_accs_lstm,Ns,Hs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}